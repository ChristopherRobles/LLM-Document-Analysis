{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8eff86-8091-4ea5-821d-e9112c68c538",
   "metadata": {},
   "source": [
    "# Christopher Robles\n",
    "# PID: 5685818\n",
    "# Date: 3/26/2025\n",
    "# LLM Document Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c9470b-c466-4730-a064-d783515f4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import ollama\n",
    "import json\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e497e682-34c9-4848-866f-95200f50f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_latest_10k(cik):\n",
    "    \"\"\"\n",
    "    Fetch the latest 10-K filing for a given company using its CIK.\n",
    "    \n",
    "    Args:\n",
    "    cik (str): The CIK (Central Index Key) of the company.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the 10-K URL and the filing date if found, else (None, None).\n",
    "    \"\"\"\n",
    "    cik = cik.zfill(10)\n",
    "    url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "    headers = {\"User-Agent\": \"Chris (ch379229@ucf.edu)\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        recent_filings = data[\"filings\"][\"recent\"]\n",
    "\n",
    "        # Find the latest 10-K filing\n",
    "        for i, form in enumerate(recent_filings[\"form\"]):\n",
    "            if form == \"10-K\":\n",
    "                accession_num = recent_filings[\"accessionNumber\"][i].replace(\"-\", \"\")\n",
    "                primary_doc = recent_filings[\"primaryDocument\"][i]\n",
    "                filing_date = recent_filings[\"filingDate\"][i]\n",
    "                ten_k_url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{accession_num}/{primary_doc}\"\n",
    "                print(f\"Latest 10-K URL: {ten_k_url}\\n\")\n",
    "                return ten_k_url, filing_date\n",
    "\n",
    "        # If no 10-K filing is found, return None\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching SEC data: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "866ff13b-b898-4a7d-b5c8-9364dbd24606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_10k_to_file(url, output_file, save_as=\"html\"):\n",
    "    \"\"\"\n",
    "    Save the 10-K document from the given URL to a local file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers={\"User-Agent\": \"ch379229@ucf.edu\"})\n",
    "        response.raise_for_status()\n",
    "\n",
    "        extension = \"html\" if save_as == \"html\" else \"txt\"\n",
    "        output_path = f\"{output_file}.{extension}\"\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as file:\n",
    "            if save_as == \"html\":\n",
    "                file.write(response.text)\n",
    "            elif save_as == \"txt\":\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                plain_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "                file.write(plain_text)\n",
    "\n",
    "        print(f\"10-K document saved to {output_path}\")\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving 10-K: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8543256f-7f5a-4910-ae69-6cd430044228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_focused_section(text):\n",
    "    \"\"\"\n",
    "    Extract sections of the 10-K that are likely to contain new product announcements or innovations\n",
    "    using a broader range of keywords.\n",
    "    \"\"\"\n",
    "    # Expanded keywords to capture broader discussions around innovations, strategic initiatives, etc.\n",
    "    keywords = r\"(innovation|strategic initiative[s]?|research and development|future offering[s]?|pipeline|business strategy)\"\n",
    "    match = re.search(keywords, text, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        start_idx = match.start()\n",
    "        # Capture more text around the keyword match for better context\n",
    "        return text[start_idx:start_idx + 3000]  # Return more text to provide context\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3f5da8-b5a2-456d-b53f-1c8e568932c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_and_ticker(text):\n",
    "    \"\"\"\n",
    "    Use regex to extract the company name and stock ticker from the 10-K text.\n",
    "    \"\"\"\n",
    "    # Pattern for company name (looks for the company name under \"Commission File Number\")\n",
    "    company_name_pattern = r\"(?:Commission File Number.*?[\\r\\n]+)([A-Z\\s&]+(?:CORPORATION|INCORPORATED|COMPANY|LIMITED|LLC)?)\"\n",
    "    company_name_match = re.search(company_name_pattern, text, re.IGNORECASE)\n",
    "    company_name = company_name_match.group(1).strip() if company_name_match else \"Unknown\"\n",
    "    \n",
    "    # Pattern for stock ticker (looks for \"Trading Symbol\" or similar terms)\n",
    "    ticker_pattern = r\"(?:Trading Symbol.*?[:\\s]+)([A-Za-z]+)\"\n",
    "    ticker_match = re.search(ticker_pattern, text, re.IGNORECASE)\n",
    "    stock_ticker = ticker_match.group(1).strip() if ticker_match else \"Unknown\"\n",
    "\n",
    "    return company_name, stock_ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59c504fe-1411-4d54-bdf0-593e33d95520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_response(response):\n",
    "    \"\"\"\n",
    "    Cleans the LLM response by extracting only the product names and descriptions,\n",
    "    ignoring the think process or extra explanations.\n",
    "    \"\"\"\n",
    "    # Define a pattern to match bullet points or product-related information\n",
    "    product_pattern = r\"(?<=\\*\\*).*?(?=\\*\\*)\"  # Matches between bold items like **Product Name**\n",
    "\n",
    "    # Find all matches of product names or descriptions in the response\n",
    "    products = re.findall(product_pattern, response)\n",
    "\n",
    "    # Return a clean list of products or a placeholder if no matches are found\n",
    "    return products if products else [\"No products found\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "271b56bf-98d8-493d-80e2-845d5f030948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llm_for_field(query):\n",
    "    \"\"\"\n",
    "    Simulate querying an LLM for extracting specific details.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(model=\"deepseek-r1:1.5b\", messages=[{\"role\": \"user\", \"content\": query}]).message.content\n",
    "\n",
    "    # Clean the response to extract product names/descriptions\n",
    "    clean_response = clean_llm_response(response)\n",
    "    \n",
    "    return clean_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a64904c8-1f6c-44ca-af08-22c6afca8f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_information_10k(ten_k_url):\n",
    "    \"\"\"\n",
    "    Extract company name, stock ticker, and product details from the 10-K document.\n",
    "    \"\"\"\n",
    "    # Fetch the 10-K document content\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Chris Robles (ch379229@ucf.edu)\",\n",
    "        \"Referer\": \"https://www.sec.gov\"\n",
    "    }\n",
    "    response = requests.get(ten_k_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error fetching 10-K document:\", response.status_code)\n",
    "        return None\n",
    "    \n",
    "    # Parse the document content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # Step 1: Extract CIK number from the text using regex\n",
    "    cik_match = re.search(r\"(?<=CIK:)\\s*\\d{10}\", text)\n",
    "    if cik_match:\n",
    "        cik_str = cik_match.group().strip()\n",
    "        \n",
    "        # Use CIK to get the company name and ticker from the JSON file\n",
    "        company_name, stock_ticker = get_company_info_by_cik(cik_str, company_data)\n",
    "    else:\n",
    "        company_name, stock_ticker = \"Unknown\", \"Unknown\"\n",
    "\n",
    "    # Step 2: Extract broader sections for product mentions (unchanged)\n",
    "    focused_text = extract_focused_section(text)\n",
    "    if not focused_text:\n",
    "        print(\"No specific product-related discussions found in the filing.\")\n",
    "        return company_name, stock_ticker, \"N/A\", \"N/A\"\n",
    "\n",
    "    # Step 3: Ask the LLM to extract details about product innovations (unchanged)\n",
    "    new_product_query = f\"Extract the names of innovations or future products from the following text:\\n{focused_text}\"\n",
    "    new_product = ask_llm_for_field(new_product_query)\n",
    "\n",
    "    product_description_query = f\"Provide a short description (max 180 characters) of the future products from the following text:\\n{focused_text}\"\n",
    "    product_description = ask_llm_for_field(product_description_query)\n",
    "\n",
    "    return company_name, stock_ticker, new_product, product_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cca7e958-1cf8-482d-96f2-b575dd904d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(data, filename=\"output.csv\"):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    \n",
    "    with open(filename, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write header only if the file is newly created\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"Company Name\", \"Stock Name\", \"Filing Time\", \"New Product\", \"Product Description\"])\n",
    "        \n",
    "        # Write data rows\n",
    "        for row in data:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db7cad8f-ab71-48f8-b1c1-8dc22677168e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest 10-K URL: https://www.sec.gov/Archives/edgar/data/0001174922/000117492225000039/wynn-20241231.htm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    # Set the CIK (Central Index Key) for the company whose 10-K filing we want to fetch\n",
    "    cik = \"1174922\"\n",
    "    # previously used cik:\n",
    "    # \"789019\", \"320193\", \"1045810\", \"1326801\", \"1067983\", \"1730168\", \"1744489\", \"1101239\", \"1315098\",\n",
    "    # \"1609711\", \"1818874\", \"1141391\", \"1108524\", \"858877\", \"1321655\", \"1075531\", \"1174922\"\n",
    "    result = fetch_latest_10k(cik)\n",
    "\n",
    "    if result:\n",
    "        \n",
    "        # If a result is returned, unpack the 10-K URL and filing time\n",
    "        ten_k_url, filing_time = result\n",
    "        company_name, stock_name, new_product, product_description = extract_information_10k(ten_k_url)\n",
    "        \n",
    "        if company_name:\n",
    "            data = [[company_name, stock_name, filing_time, new_product, product_description]]\n",
    "            save_to_csv(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the main function when the script is run directly\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c077aaef-f9ec-4d3a-88a0-04446c083f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(input_file, output_file):\n",
    "    with open(input_file, 'r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        data = list(reader)\n",
    "\n",
    "    new_data = []\n",
    "    for row in data:\n",
    "        products = ast.literal_eval(row['New Product'])\n",
    "        for product in products:\n",
    "            new_row = row.copy()\n",
    "            new_row['New Product'] = product\n",
    "            new_data.append(new_row)\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        fieldnames = ['Company Name', 'Stock Name', 'Filing Time', 'New Product', 'Product Description']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(new_data)\n",
    "\n",
    "# Usage\n",
    "process_csv('output.csv', 'final_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a72afce-bb9f-4330-b07d-7a7b2c7c0ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
